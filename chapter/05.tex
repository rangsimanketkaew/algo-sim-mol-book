% LaTeX source for ``Algorithms for Computer Simulation of Molecular Systems''
% Copyright (c) 2023 รังสิมันต์ เกษแก้ว (Rangsiman Ketkaew).

% License: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)
% https://creativecommons.org/licenses/by-nc-nd/4.0/

\chapter{การคำนวณวิทยาศาสตร์สมรรถนะสูง}
\label{ch:high_perf_comp}

%----------------------------------------
\section{ทำไมต้อง High-Performance Computing ?}
%----------------------------------------

%----------------------------------------
\section{ทำไมต้องคำนวณแบบขนาน (Parallel Computing) ?}
%----------------------------------------

%----------------------------------------
\section{ทักษะและเครื่องมือสำหรับการเขียนโปรแกรมสำหรับ HPC}
%----------------------------------------

- Architecture
- Memory management
- Threading, multithreading
- Block

- Parallel computing (SPMD)
- Shared memory: OpenMP
- Distributed memory: MPI
- Implementations: OpenMPI, Intel MPI, MVAPICH

- Intel ecosystem
- OpenMP compiler: icc, ifort
- MPI compiler: mpicc, mpiicc (for Intel C compiler), mpicxx (for C++), mpiifort (for Fortran)

- Cloud computing (bonus)

- Server and database

- Networking

Essential skills for coding GPU

- Intermediate/advanced C or C++ skills

- Programming model: Kernels, thread hierarchy, memory hierarchy, heterogeneous hierarchy, asynchronous SIMT

- CUDA
- Understand CUDA operation:
1. Declare and allocate host and device memory.
2. Initialize host data.
3. Transfer data from the host to the device.
4. Execute one or more kernels.
5. Transfer results from the device to the host.
- CUDA C and CUDA C++ API
- Compiler: nvcc

%----------------------------------------
\section{การทำให้เกิดเมทริกซ์รูปทแยง (Matrix Diagonalization)}
\idxboth{การทำให้เกิดเมทริกซ์รูปทแยง}{Matrix Diagonalization}
%----------------------------------------

ในหัวข้อนี้ผู้อ่านจะได้ศึกษาการทำให้เกิดเมทริกซ์รูปทแยงหรือ Matrix Diagonalization (ผมขอเรียกสั้น ๆ ว่า MatDiag เพื่อความสะดวก)
ในการคำนวณแบบขนาน (Parallel Computing) สำหรับการคำนวณทางเคมีควอนตัม โดย MatDiag คือการดำเนินการ (Operation)
ทางพีชคณิตเชิงเส้นแบบหนึ่งซึ่งถูกใช้อย่างแพร่หลายโดยเฉพาะในงานวิจัยด้านการคำนวณทางวิทยาศาสตร์ แน่นอนว่าโปรแกรมทางเคมีควอนตัมนั้นก็ใช้
MatDiag เยอะมาก ๆ ซึ่งมีความซับซ้อนเชิงการคำนวณอยู่ที่ $O(n^3)$ ทำให้เกิดปัญหาคอขวดและทำให้การคำนวณของระบบที่มีขนาดใหญ่นั้นช้ามาก ๆ
เพื่อแก้ปัญหาดังกล่าวจึงได้มีการพัฒนาเทคนิคและไลบรารี่ที่จะเข้ามาช่วยเราในการทำ MatDiag ได้แบบขนาดหรือ Parallel ซึ่งช่วยให้การทำ
MatDiag เร็วขึ้นถึง 50\% เลยทีเดียว เริ่มต้นเรามีเมทริกซ์ที่มีขนาดใหญ่และสมาชิกส่วนใหญ่ของเมทริกซ์นั้นมีค่าไม่เท่ากับ 0 (Nonzero Elements)
ซึ่งเราจะเรียกเมทริกซ์ประเภทนี้ว่าเมทริกซ์แบบเต็ม (Full Matrix) หรือเมทริกซ์แบบแน่น (Dense Matrix) ก็ได้ โดยเราสามารถแทน Dense
Matrix ได้โดยใช้รูปแบบที่เรียกว่า Block Cyclic ในการทำการคำนวณแบบขนานด้วยวิธี Message-passing Interface (MPI) ซึ่งเป็นการ%
กำหนดการกระจาย Dense Matrix ไปยังหน่วยประมวลผล (Processor) แต่ละตัวของเครื่องคำนวณ (Compute Node) ในคลัสเตอร์คอมพิวเตอร์



ให้ดูที่ภาพแรกก่อนซึ่งเป็นการเปรียบเทียบการกระจายแบบ \enquote{Cyclic} และแบบ \enquote{Block} สำหรับเวกเตอร์ 1 มิติและเมทริกซ์
2 มิติ (ภาพด้านล่างผมคัดลอกมาจาก Tutorial \enquote{Introduction to Parallel} ของ Blaise Barney แห่งสถาบัน LLNL)
โดยสีแต่ละสีของแต่ละช่องนั้นจะเป็นการบ่งบอกถึง Processor ที่ต่างกัน และแต่ละ Segment นั้นจะบ่งบอกถึงสัดส่วน (portion) ของ Dense
Matrix ที่ถูกกำหนดและแบ่งเข้าไปใน Local Memory ของแต่ละ Processor สำหรับการแยกออกเป็นส่วน ๆ แบบหมุนวน (Cyclic Decomposition)
ของเมทริกซ์นั้นสามารถทำได้คือเราจะทำการ Distribute แต่ละแถวหรือแต่ละคอลัมน์ไปยัง Processor ที่แตกต่างกัน (เราอาจจะแบ่งเป็นทีละคู่ก็ได้
เช่น แบ่งทุก ๆ 2 แถว) ในทางตรงข้ามนั้น วิธีการแบ่งแบบ Block Representation นั้นจะเป็นการแยกเมทริกซ์ออกเป็นเมทริกซ์ย่อย ๆ จำนวน
$N$ เมทริกซ์ (เรียกว่า Submatrices ก็ได้) โดยไม่สนใจว่าขนาดของแต่ละ Block นั้นจะต้องมีขนาดที่เท่ากัน ซึ่งแต่ละ Submatrix นั้นจะถูก%
ส่งต่อไปยัง Processor แต่ละตัว สรุปคือการแบ่งแบบ Cyclic นั้นเป็นการนำการแบ่งแบบ Block มาทำซ้ำ ๆ กันไปแบบละเอียดกว่า ซึ่งจะทำให้%
เราได้ Block ที่มากกว่า

แล้วข้อดีหรือข้อเสียของทั้งสองวิธีนี้คืออะไร? เราจะเห็นได้ว่า Cyclic Distribution นั้นจะเหมาะกว่าการกระจายเมทริกซ์แบบเท่า ๆ กัน (evenly)
แต่ว่าจะมีการจัดการเมทริกซ์ที่ทำได้แย่กว่าเพราะว่าจะต้องมีการสื่อสาร (Communication) ระหว่าง Processor และระหว่าง Compute Node
ในการส่งต่อข้อมูลของ Matrix Element ที่ถูกคำนวณด้วย Processor ที่อยู่ใกล้กันซึ่งในทางตรงกันข้ามนั้นการ Communication ใน Block
Representation Matrix นั้นจะทำได้ดีกว่าเพราะว่ามันมีการแบ่งแบบต่อเนื่องบนหน่วยความจำ แต่ว่าถ้าหากว่า Matrix ของเราเป็นแบบ Sparse
Matrix หรือเมทริกซ์ที่มีสมาชิกส่วนใหญ่เป็น 0 นั้นก็อาจจะเกิดปัญหาเช่น Load Balancing ได้

เพื่อรวมข้อดีของทั้งสองวิธีไว้จึงได้มีการแบ่งแบบ Block Cyclic Distribution ซึ่งก็คือเป็นการแยกเมทริกซ์ออกเป็น Block เล็ก ๆ แล้วก็ทำการกระจาย
Blocks เหล่านี้แบบหมุนวน (Cyclically) ไปยัง Processors ทุกตัว โดยให้ดูตัวอย่างของ Block Cyclic Distribution ตามภาพที่ 2
สำหรับวิธีการแบ่งที่มีประสิทธิภาพที่สุดนั้นก็คือจำนวนของ Block ที่ถูกแบ่งออกมานั้นนั้นจะต้องเท่ากับจำนวนของ Processors
(หมายความว่ามีมิติเท่ากัน $N_{\text{row}} \times N_{\text{col}}$)
โดย N คือจำนวน Processors ซึ่งตามภาพที่สองนั้นเราจะเห็นได้ว่า Block ที่มีสีเหมือนกันนั้นคือถูกคำนวณบน Processor เดียวกัน (ดู Local View)
และขนาดของแต่ละ Block ควรจะต้องเท่ากันด้วย ส่วนภาพที่ 3 นั้นแถมให้ครับ ซึ่งก็เป็นอีกตัวอย่างของการแยกเมทริกซ์ออกเป็นส่วน ๆ (Decomposition)
แบบ Block Cyclic Distribution

%----------------------------------------
\section{การวัดประสิทธิภาพไลบรารี่สำหรับ Matrix Diagonalization}
%----------------------------------------

เราจะมาศึกษาการวัดประสิทธิภาพของไลบรารี่ ScaLAPACK กับ ELPA ซึ่งทั้งสองตัวนี้เป็นไลบรารี่สำหรับการทำ MatDiag แบบขนานซึ่งได้รับความนิยม%
ในการนำมาใช้ในการเขียนโปรแกรมที่รันบนซุปเปอร์คอมพิวเตอร์ เช่น โปรแกรมสำหรับการทำงานวิจัยด้านวิทยาศาสตร์ โดยเราได้ศึกษากันไปแล้วว่า%
ถ้าหากเรามี Dense Matrix A ที่ถูกกระจายหรือแบ่งไปคำนวณบน Processors แต่ละตัวด้วยวิธี MPI โดยการใช้ Block Cyclic Distribution
สิ่งที่เรามักจะทำกันต่อจากนั้นก็คือการ Diagonalize ซึ่งเราต้องพยายามทำให้มัน Efficient ที่สุดโดยการทำ Diagonalization นั้นคือการหา%
คำตอบของปัญหาค่าไอเกน (Eigenvalue Problem) ดังต่อไปนี้ AX = XL โดยที่ X คือเมทริกซ์ที่บรรจุ Eigenvector ของ A ไว้ ส่วน L
นั้นคือเมทริกซ์ที่บรรจุค่า Eigenvalue ซึ่งเมทริกซ์ L นี้เองที่เราจะต้องมาทำการหาเพราะมันเป็น Diagonal เมทริกซ์จริง ๆ ของ A

ในไลบรารี่ ScaLAPACK นั้นมีอัลกอริทึม 3 ตัวที่เราสามารถนำมาใช้ในการทำ Diagonalize Matrix ที่เป็น Real Symmetry Matrix
(เมทริกซ์ที่มีความสมมาตรและมีเพียงแค่ค่าจริงเป็นสมาชิกเท่านั้น) นั่นคือ p?syevd, p?syevx, และ p?syevr โดยที่ ? นั้นแทนด้วยค่าที่บ่งบอก%
ถึงประเภทของข้อมูลในเมทริกซ์ เช่น ถ้าแทน ? ด้วย d จะหมายถึงเมทริกซ์นั้นเก็บข้อมูลประเภท Double Precision
โดยทั้งสามอัลกอริทึมนี้ก็จะใช้วิธีที่แตกต่างกันไป เช่น syevd จะใช้วิธีแบ่งแยกและเอาชนะ (Divide and Conquer) ซึ่งผลการทดสอบที่แสดง%
ด้านล่างนั้นได้มาจากการใช้ p?syevd นั่นเองครับ โดยสรุปสั้น ๆ คือวิธี Divide and Conquer นั้นจะเริ่มด้วยการทำการลดรูปเมทริกซ์ (Reduction)
A ให้เป็น Tridiagonal Matrix ก่อนโดยใช้การแปลง Householder หลังจากนั้นก็ทำการหา Tridiagonal Eigenvalue ด้วยอัลกอริทึม
Divide and Conquer แล้วก็ทำการแปลงย้อนกลับไปให้ได้เป็น Eigenvector ออกมา

ตามที่ได้อธิบาย p?syevd ของไลบรารี่ ScaLAPACK ไปแล้วนั้น ลำดับต่อไปคือไลบรารี่ยอดฮิตอีกตัวที่ได้รับความนิยมในการนำมาใช้ในโปรแกรมทาง%
เคมีควอนตัมหลายตัวด้วยกัน เช่น NWChem และ CP2K นั่นก็คือไลบรารี่ ELPA จริง ๆ แล้ว ELPA นั้นเอาอัลกอริทึมใน ScaLAPACK มาปรับปรุง%
อีกทีนึงเพื่อให้มีประสิทธิภาพมากขึ้น โดยจะใช้เทคนิค Direction Transformation ของ Tridiagonal Form ซึ่งก็มีความซับซ้อนพอสมควร
เอาเป็นว่าทั้ง ScaLAPACK กับ ELPA ก็สามารถนำมาใช้ได้ทั้งคู่ แล้วก็ Interface นั้นมีความคล้ายคลึงกันมาก สิ่งที่แตกต่างอีกอย่างหนึ่งก็คือ ELPA
นั้นมีความ General มากกว่าตรงที่สามารถใช้กับ Fortran Kernel ได้บนหลากหลายสถาปัตยกรรมมากกว่า เช่น ถ้า Kernel ของ CPU เป็นแบบใหม่ ๆ
เช่น AVX, AVX2, หรือ AVX-512 นั้น ELPA ก็จะรองรับ คราวนี้เรามาดูการทำ Benchmark หรือการวัดประสิทธิภาพของไลบรารี่ทั้งคู่นี้กัน
ปกติแล้วการทำ MatDiag นั้นจะขึ้นอยู่กับปัจจัยหลายตัว โดยหลัก ๆ แล้วมีดังนี้

\begin{itemize}[topsep=0pt,noitemsep]
  \setlength\itemsep{1em}
  \item ขนาดของ Matrix

  \item โครงสร้างของ Matrix

  \item จำนวน Processors ของเครื่องที่ใช้ในการรันหรือคำนวณ Diagonalization

  \item ขนาดของ MPI Block Size สำหรับ Matrix

  \item อัลกอริทึมที่ใช้ในการทำ Diagonalization
\end{itemize}

สำหรับตัวอย่างที่เราจะมารันทดสอบ Benchmark กันนัั้นก็คือเมทริกซ์จตุรัสขนาด $5888 \times 5888$ กับขนาด $13034 \times 13034$
ตามลำดับ โดยใช้โปรแกรม CP2K (โปรแกรมทางเคมีควอนตัม) ซึ่งจริง ๆ แล้วก็คือระบบที่เป็นโมเลกุลน้ำ (Water Cluster) 128 โมเลกุลนั่นเอง
สำหรับเครื่องซุปเปอร์คอมพิวเตอร์ที่ใช้ในการทดสอบนั้นคือ Cray XC40 ซึ่งมีสเปคคือแต่ละโหนดนั้นจะมี 12-core Intel Xeon E5-2690v3 Processors
ทั้งหมด 2 ตัว และมี Memory คือ 64 GB (DDR4) สำหรับผลการทดสอบนั้นก็ดูตามภาพด้านล่างได้เลย แกน y คือประสิทธิภาพที่ได้ส่วนแกน $x$
นั้นคือจำนวนของ Node ของ Cray XC40 โดย SL คือ ScaLAPACK, ส่วน QR นั้นคือเทคนิค Decomposition แบบหนึ่งที่เราเอาเข้ามาช่วยใน%
การเพิ่มประสิทธิภาพการทำ Diagonalization ของ ELPA นั่นเอง ภาพด้านซ้ายนั้นคือระบบที่เมทริกซ์ขนาดใหญ่ส่วนด้านขวาเมทริกซ์ขนาดเล็ก
สรุปคือจากการทดสอบนั้นก็คือ ELPA ชนะขาดลอยในการทำ Diagonalization แบบขนานด้วย MPI ซึ่ง ELPA ทำประสิทธิภาพได้ดีกว่า SL
ประมาณ 60-80\% เลยทีเดียว

%----------------------------------------
\section{การประยุกต์ใช้ Matrix Diagonalization}
%----------------------------------------

หนึ่งในการประยุกต์ใช้ Matrix Diagonalization ในโปรแกรมเคมีเชิงคำนวณก็คือการคำนวณพลังงานของออร์บิทัล (Orbital Energies)
ซึ่งเป็นเทอมที่สำคัญมาก ๆ ในทางเคมีเพราะว่าเป็นตัวที่เราจะนำมาใช้ในการศึกษาโมเลกุล โดยหลาย ๆ คนที่เคยวิชาเคมีอินทรีย์เชิงฟิสิกส์มานั้นก็น่า%
จะเคยผ่านการใช้ Huckel Model Theory ในการคำนวณหาพลังงานของออร์บิทัลของโมเลกุลเคมีอินทรีย์ (สารประกอบไฮโดรคาร์บอน) แบบง่าย ๆ
กันมาแล้ว เช่น โมเลกุลเบนซีน ซึ่งวิธีที่เราจะใช้ในการคำนวณหา Orbital Energy นั้นเราจะต้องทำการกำหนดฟังก์ชันคลื่นที่ใช้อธิบาย MO สำหรับ
$\pi$ Electron ขึ้นมาก่อน ซึ่งเราสามารถใช้ผลรวมเชิงเส้นของ Atomic Orbitals ได้ เช่น สำหรับโมเลกุลเบนซีน เขียนได้ดังนี้

\begin{equation}
  \phi_{n} = \sum_{i=1}^{6} C_{i} \chi_{i}
\end{equation}

โดยที่ $C_{i}$ คือ Molecular Orbital Coefficients และ $\chi$ คือ Basis Function คราวนี้เราสามารถใช้สมการ Eigenfunction

\begin{equation}
  HC = \epsilon C
\end{equation}

\noindent ในการหา $\epsilon$ ซึ่งเป็นพลังงานของแต่ละออร์บิทัลได้ โดยที่ H คืออินทิกรัลของ Basis Function โดยหน้าตา H, C และ
$\epsilon$ นั้นจริง ๆ แล้วก็คือ Square Matrix ดี ๆ นี่เอง โดยสามารถดูตัวอย่างของทั้งสามเมทริกซ์นี้สำหรับกรณีโมเลกุลเบนซีนได้ตามภาพที่ 1
โดย $\alpha$ กับ $\beta$ นั้นก็คือ Coefficient ของแต่ AO แต่ละอันนั่นเอง เช่น อะตอมคาร์บอนตัวที่ 1 นั้นก็จะมี Interaction
กับคาร์บอนที่ 2 กับ 6 ซึ่งการแก้สมการ Secular Equation นี้เราสามารถทำ Diagonalization ได้ โดยจัดรูปสมการเป็น%
\footnote{ต้องทำความเข้าใจกันก่อนว่าทฤษฎี H\"{u}ckel Model นั้นจะ Treat หรือสนใจเฉพาะ MO ของ $\pi$ Electron สำหรับโมเลกุล%
ที่เป็นแบบ $\pi$-conjugated เท่านั้น}

\begin{equation}
  (H - \epsilon)C = 0
\end{equation}

หนึ่งในวิธีที่หลายคนมักจะนำมาใช้ในการทำ Diagonalization นั้นก็คือ Jacobi Method แต่ว่าวิธีนี้มีจุดอ่อนคือมันไม่ได้ทำการแยกตัวประกอบของ
Secular Equation ออกเป็นเทอม ๆ จึงทำให้เราไม่มี Main-diagonal Block แล้ว Main-diagonal Block คืออะไร? ทำไมถึงสำคัญ?
ประเด็นก็คือการที่เราทำ Diagonalization นั้นมันจะมีวิธีบางอย่างที่สามารถจัดการเมทริกซ์ให้อยู่ในรูปที่เกิดจากการประกอบกันระหว่าง Diagonal
Matrix หลาย ๆ อันได้และสมาชิกของเมทริกซ์ที่อยู่นอก Main-diagonal Block นั้นจะต้องเป็น 0 ด้วย เช่นให้ดูตามภาพที่ 2 ถ้าใครยังไม่เข้าใจ%
ให้ดูภาพที่ 3 จะได้เห็นภาพของ Diagonal Block ชัดขึ้น ซึ่งกลับมาที่ Jacobi Method ที่ไม่ได้ทำการแยก Block Diagonal Matrix
ออกมาให้เรา ซึ่งนี่เป็นสาเหตุที่ทำให้ Symmetry ของโมเลกุลนั้นหายไประหว่างการทำ Diagonalization และทำให้ออร์บิทัลที่เป็นแบบ Degenerate
(ออร์บิทัลที่มีพลังงานเท่ากัน) นั้นหายไปด้วย ซึ่งในความเป็นจริงโมเลกุลเบนซีนจะต้องมีบางออร์บิทัลที่มีพลังงานเท่ากัน ตามภาพที่ 4
ดังนั้นสิ่งที่เราต้องการคือ Diagonalization Method ที่สามารถให้ Main-diagonal Block ที่มีให้ Degenerate MOs อย่างไรก็ตามในการ%
คำนวณทางเคมีควอนตัมนั้นถ้าหากเราใช้ทฤษฎีอื่น ๆ นั้นก็จะมีการนิยามและคำนวณหาพลังงาน MO (Eigenvalues) ที่แตกต่างกันไปและซับซ้อนมากขึ้น
แต่หลัก ๆ แล้วก็จะต้องมีการทำ Diagonalization อยู่ดีครับ

%----------------------------------------
\section{การวัดประสิทธิภาพโปรแกรม Ab initio Molecular Dynamics}
%----------------------------------------

ในหัวข้อนี้เราจะมาดูรายละเอียดขั้นตอนการประเมินประสิทธิภาพ (ความเร็ว) ของโปรแกรม Ab initito Molecular Dynamics (AIMD) กันครับ
ในปัจจุบันนั้นมีโปรแกรมที่สามารถรันการคำนวณ AIMD หลายโปรแกรมมาก ๆ โดยโปรแกรมที่ได้รับความนิยม เช่น CPMD, Quantum Espresso,
CP2K, NWChem, VASP โดยส่วนตัวของผมเองนั้นก็ได้มีโอกาสวัดประสิทธิภาพ (Benchmarking) ความเร็วโปรแกรมที่กลุ่มวิจัยที่ผมมาศึกษาต่อนั้นใช้
ก็คือโปรแกรม CP2K ว่าทำงานได้เร็วและมี Speed-up Scaling มากน้อยแค่ไหนบน Distributed System ของ Supercomputer

แน่นอนว่าขั้นตอนเริ่มต้นนั้นก็คือการเตรียมโครงสร้างของระบบโมเลกุลที่ต้องการนำมาใช้ในการทดสอบ แล้วก็รัน Simulation โดยการเปลี่ยนจำนวนของ
Compute Nodes โดยเริ่มจาก 1 Note และเพิ่มจำนวนเป็น 2, 4, 16, ..., 64 ตามลำดับ ซึ่งตามทฤษฎีแล้วนั้นความเร็วของ Software
ที่ได้ควรจะต้องเร็วขึ้นตามจำนวนเท่าของการเพิ่มจำนวน Compute Node

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{fig/perf-scaling-cp2k.pdf}
  \caption{Speed-up Scaling ของโปรแกรม CP2K ตามจำนวน Compute Nodes ของซุปเปอร์คอมพิวเตอร์ CSCS Piz Daint}
  \label{fig:perf_scaling_cp2k}
\end{figure}

แต่ในความจริงนั้นโปรแกรมของเราไม่ได้ทำงานเร็วตามทฤษฎีและ Speed-up Scaling ก็ไม่ได้เป็น Linear Scaling หรอกครับ
มันมีปัญหาเยอะและมีหลายเหตุผลที่ทำให้เกิดคอขวด (Bottleneck) โดย Scaling ที่ได้ก็ตามกราฟในรูปที่ \ref{fig:perf_scaling_cp2k}
จะเห็นว่าพอเราเพิ่มจำนวน Compute Node จาก 24 $\rightarrow$ 32 แล้ว Efficiency (คำนวณจาก Speedup หารด้วยจำนวน Node)
เริ่มดรอปลง

คำถามคือสิ่งที่ผมทำนี้มันถูกต้อง 100\% ไหม จริง ๆ แล้วไม่ถูก 100\% เพราะว่าการที่เรารัน MD Simulation หลาย ๆ ครั้งถึงแม้ว่าจะใช้ Input
ไฟล์เดียวกันและเหมือนกันทั้งหมดนั้น สิ่งที่เกิดขึ้นคือผลการคำนวณที่ได้จะไม่มีทางที่เหมือนกัน นั่นก็เพราะว่า Algorithm ของโปรแกรม MD
(เรียกได้ว่าเกือบทุกโปรแกรมเลย) จะมีการสุ่ม (Random) พารามิเตอร์บางตัวขึ้นมาก่อนใน Step แรกสุดของการคำนวณ
ซึ่งพารามิเตอร์นั้นก็คือความเร็วของอะตอมแต่ละตัวในโมเลกุลซึ่งจะถูกนำมาใช้ในการแก้สมการ Newton แบบคลาสสิคสำหรับการคำนวณหาแรง (Force)
เพื่อใช้ในการอัพเดทตำแหน่งของอะตอมแต่ละตัวใน Step ต่อไป ซึ่งเราเรียกเท่ ๆ ว่าการ Propagation
(ใน CP2K เราใช้ DFT-MD ซึ่งจะใช้ควอนตัมในการคำนวณพลังงานและแรงของอะตอมแต่ละตัว ซึ่งเรา"เคลม"และเชื่อว่าให้ผลที่แม่นยำและถูกต้องกว่าใช้ Force field)

ถึงแม้ว่าเราจะรัน Simulation ด้วยไฟล์ Input เดียวกัน 2 รอบ ผลการคำนวณนั้นจะไม่เหมือนกัน เช่น พลังงานของระบบในแต่ละ MD Step
นั้นถ้าเทียบกันแล้วจะแตกต่างกันอย่างสิ้นเชิง และแน่นอนว่าระยะเวลาจริงที่ใช้ในการคำนวณ (Simulation Time) ของแต่ละ MD Step นั้นก็ต่างกันด้วย
เหตุผลก็ตามที่ได้บอกไปคือ Initial Parameter นั้นไม่เหมือนกัน จึงทำให้การรันสองทั้งนั้นให้ผลที่ไม่เหมือนกัน

ถ้าถามว่าผิดไหมที่โปรแกรม MD ส่วนใหญ่นั้นทำการสุ่มค่าความเร็วเริ่มต้นของอะตอมแต่ละตัว คำตอบคือไม่ผิด เพราะว่าท้ายที่สุดแล้วเราสนใจระบบกรณี%
ที่มีความเป็น Ergodicity แบบสมบูรณ์แล้ว เมื่อเรารัน Simulation ไปเรื่อย ๆ จนระบบเข้าสู่สภาวะสมดุล (Equilibrium) เราสามารถอ้างได้ว่า
Configuration แต่ละตัวนั้นสามารถที่จะ Represent คุณสมบัติแบบ Microscopic ได้

\begin{lstlisting}[
  style=MyFortran,
  basicstyle=\ttfamily\footnotesize\linespread{0.5}
  ]
DO i = 1, natoms
  atomic_kind => part(i)%atomic_kind
  CALL get_atomic_kind(atomic_kind=atomic_kind, mass=mass)
  part(i)%v(1) = 0.0_dp
  part(i)%v(2) = 0.0_dp
  part(i)%v(3) = 0.0_dp
  IF (mass .NE. 0.0) THEN
     SELECT CASE (is_fixed(i))
     CASE (use_perd_x)
        part(i)%v(2) = globenv%gaussian_rng_stream%next()/SQRT(mass)
        part(i)%v(3) = globenv%gaussian_rng_stream%next()/SQRT(mass)
     CASE (use_perd_y)
        part(i)%v(1) = globenv%gaussian_rng_stream%next()/SQRT(mass)
        part(i)%v(3) = globenv%gaussian_rng_stream%next()/SQRT(mass)
     CASE (use_perd_z)
        part(i)%v(1) = globenv%gaussian_rng_stream%next()/SQRT(mass)
        part(i)%v(2) = globenv%gaussian_rng_stream%next()/SQRT(mass)
     CASE (use_perd_xy)
        part(i)%v(3) = globenv%gaussian_rng_stream%next()/SQRT(mass)
     CASE (use_perd_xz)
        part(i)%v(2) = globenv%gaussian_rng_stream%next()/SQRT(mass)
     CASE (use_perd_yz)
        part(i)%v(1) = globenv%gaussian_rng_stream%next()/SQRT(mass)
     CASE (use_perd_none)
        part(i)%v(1) = globenv%gaussian_rng_stream%next()/SQRT(mass)
        part(i)%v(2) = globenv%gaussian_rng_stream%next()/SQRT(mass)
        part(i)%v(3) = globenv%gaussian_rng_stream%next()/SQRT(mass)
     END SELECT
  END IF
END DO
\end{lstlisting}

โค้ดด้านบนคือโค้ดของ CP2K ที่ทำการกำหนด (Assign) ความเร็วให้อะตอมแต่ละตัวโดยใช้ Maxwell-Boltzmann Distribution
(เรียกอีกชื่อว่า Maxwellian Distribution) ซึ่งมีเบื้องหลังคือถูก Derived มาจาก Gaussian Distribution โดยที่มีการกำหนดค่า Random
สำหรับ Variance จาก 0 ไปถึง 1

\begin{lstlisting}[
  style=MyFortran,
  basicstyle=\ttfamily\footnotesize\linespread{0.5}
  ]
globenv%gaussian_rng_stream = rng_stream_type( &
                              name="Global Gaussian random numbers", &
                              distribution_type=GAUSSIAN, &
                              seed=initial_seed, &
                              extended_precision=.TRUE.)
\end{lstlisting}

\noindent การกำหนดความเร็วของอะตอมแบบสุ่มนั้นจะต้องมีการกำหนด Seed สำหรับการ Random ด้วยโดยดูได้ตามโค้ดด้านบน

\begin{lstlisting}[
  style=MyFortran,
  basicstyle=\ttfamily\footnotesize\linespread{0.5}
  ]
SUBROUTINE normalize_velocities(simpar, part, force_env, md_env, is_fixed)
  TYPE(simpar_type), POINTER                         :: simpar
  TYPE(particle_type), DIMENSION(:), POINTER         :: part
  TYPE(force_env_type), POINTER                      :: force_env
  TYPE(md_environment_type), POINTER                 :: md_env
  INTEGER, DIMENSION(:), INTENT(INOUT)               :: is_fixed

  REAL(KIND=dp)                                      :: ekin
  REAL(KIND=dp), DIMENSION(3)                        :: rcom, vang, vcom
  TYPE(cell_type), POINTER                           :: cell

  NULLIFY (cell)

  ! Subtract the vcom
  CALL compute_vcom(part, is_fixed, vcom)
  CALL subtract_vcom(part, is_fixed, vcom)
  ! If requested and the system is not periodic, subtract the angular velocity
  CALL force_env_get(force_env, cell=cell)
  IF (SUM(cell%perd(1:3)) == 0 .AND. simpar%angvel_zero) THEN
    CALL compute_rcom(part, is_fixed, rcom)
    CALL compute_vang(part, is_fixed, rcom, vang)
    CALL subtract_vang(part, is_fixed, rcom, vang)
  END IF
  ! Rescale the velocities
  IF (simpar%do_thermal_region) THEN
    CALL rescale_vel_region(part, md_env, simpar)
  ELSE
    ekin = compute_ekin(part)
    CALL rescale_vel(part, simpar, ekin)
  END IF
END SUBROUTINE normalize_velocities
\end{lstlisting}

\noindent เมื่อเรากำหนดหรือคำนวณความเร็วของอะตอมได้แล้ว เรามีวิธีการปรับให้ความเร็วนั้นสอดคล้องกับอุณหภูมิของระบบที่เราต้องการโดย%
ดูได้ตามโค้ดด้านบน (ดูโค้ดของโปรแกรม CP2K ในพาร์ทที่เป็น MD Motion ได้ที่
\url{https://github.com/cp2k/cp2k/tree/master/src/motion})

สรุป ถ้าหากเราอยากจะแก้ปัญหาเรื่องการไม่เท่ากันของความเร็ว เราสามารถทำได้ 2 วิธี (จริง ๆ มีวิธีอื่นอีก) คือ

\begin{enumerate}[topsep=0pt,noitemsep]
  \setlength\itemsep{1em}
  \item รัน MD Simulation ก่อน 1 ครั้ง แล้วนำความเร็วสุดท้ายที่ได้มาใช้เป็นความเร็วเริ่มต้นสำหรับการทำ Benchmark

  \item กำหนด Seed ในการสุ่มของการสร้าง Uniformly Distributed Random Number เพื่อที่ว่าเราจะได้ Gaussian Distribution
        ที่เหมือนกันทุกประการ และได้ Maxwellian Velocity ที่เหมือนกันด้วย
\end{enumerate}

ถ้าหากผู้อ่านอยากศึกษาเพิ่มเติม ผมแนะนำตามนี้ครับ

\begin{itemize}[topsep=0pt,noitemsep]
  \setlength\itemsep{1em}
  \item Understanding Molecular Simulation

  \item Computer Simulation of Liquids
        หรืออ่านวิกิพีเดียที่ \url{https://en.wikipedia.org/wiki/Maxwell%E2%80%93Boltzmann_distribution#Distribution_for_the_velocity_vector}
\end{itemize}

%----------------------------------------
\section{การเขียนโปรแกรมเคมีควอนตัมบน GPU}
%----------------------------------------

%----------------------------------------
\subsection{อัลกอริทึมสำหรับการคำนวณ Self-Consisten Field แบบผสมบน CPU และ GPU}
%----------------------------------------

ในหัวนี้ผู้อ่านจะได้ศึกษาความก้าวหน้าเกี่ยวกับงานวิจัยที่พัฒนาวิธีในการคำนวณ Self-Consistent Field (SCF) เพื่อให้สามารถรันได้บนทั้ง CPU
และ GPU แบบพร้อม ๆ กัน ซึ่งช่วยลดระยะเวลาในการคำนวณของวิธี Hartree-Fock (HF) ไปได้เยอะมาก ๆ ซึ่งการที่เราสามารถคำนวณ HF
ได้เร็วขึ้นนั้น ก็ทำให้การคำนวณวิธีอื่น ๆ ที่ใช้ HF เป็น Single Reference นั้นเร็วขึ้นตามไปด้วย เช่น วิธี Post-HF ต่าง ๆ

เริ่มต้นเลยต้องเข้าใจก่อนว่าในการคำนวณทางเคมีควอนตัมที่ต้องมีการใช้วิธี HF เพื่อมาสร้าง Single Wavefunction Reference นั้นจะมีส่วนที่%
สิ้นเปลืองการคำนวณมากที่สุดอยู่ด้วยกัน 2 ส่วน นั่นคือ

\begin{enumerate}[topsep=0pt,noitemsep]
  \setlength\itemsep{1em}
  \item การคำนวณอินทิกรัลของการผลักระหว่างอิเล็กตรอน (Electron Repulsion Integral หรือ ERI)

  \item การสร้าง Fock Matrix ขึ้นมาจาก ERI และ Density Matrix
\end{enumerate}

ถ้าหากว่าเราไล่ดูตามไทม์ไลน์ของการพัฒนาวิธีการทางโครงสร้างเชิงอิเล็กโทรนิกส์เพื่อให้สามารถไปรันได้บน GPU นั้น บทความงานวิจัยแรกสุดเลย%
ที่มีการศึกษาการคำนวณ ERI สำหรับออร์บิทัล s กับออร์บิทัล p นั้นคือเมื่อปี 2008 โดย K. Yasuda แล้วหลังจากนั้นก็มีงานวิจัยที่พัฒนาต่อยอดที่%
เกี่ยวกับการ GPU ตามมาอีกเยอะมาก

ในการคำนวณ HF นั้น เริ่มต้นเราจะต้องสร้าง Fock Matrix ขึ้นมาก่อน ซึ่งเมทริกซ์อันนี้เป็นแบบ 2 มิติ จำนวนสมาชิกของเมทริกซ์เท่ากับจำนวน
Basis Functions คูณกัน โดย Fock Matrix คำนวณได้จากการนำ Hamiltonian Matrix มารวมกันกับ Density Matrix คูณกับ ERI
โดยสมการของการสร้าง Fock Matrix $(F_{\mu v})$ มีดังนี้

\begin{equation}
  F_{\mu \nu}
  =
  H_{\mu \nu}
  + \sum_{\lambda \sigma} D_{\lambda \sigma}\left[(\mu \nu \mid \lambda \sigma)
    - \frac{1}{2}(\mu \lambda \mid \nu \sigma)\right]
\end{equation}

\noindent โดยที่ $H_{\mu v}$ และ $D_{\mu v}$ นั้นคือสมาชิกของ Core Hamiltonian Matrix และ Density Matrix ตามลำดับ
แล้วก็ $(\mu v \mid \lambda \sigma)$ นั้นคือ Notation ที่เราใช้เพื่อแทนอินทิกรัล ERI ซึ่งมีสมการดังต่อไป

\begin{equation}
  (\mu \nu \mid \lambda \sigma)
  =
  \iint \varphi_\mu\left(\boldsymbol{r}_1\right) \varphi_\nu\left(\boldsymbol{r}_1\right)
  \frac{1}{r_{12}}
  \varphi_\lambda\left(\boldsymbol{r}_2\right) \varphi_\sigma\left(\boldsymbol{r}_2\right)
  \mathrm{d} \boldsymbol{r}_1 \mathrm{~d} \boldsymbol{r}_2
\end{equation}

\noindent โดยเราใช้เบสิสเซทที่เป็นแบบ Contracted Gaussian Basis Functions $\varphi(r)$ ซึ่งมีเลขดัชนี $\mu, v, \lambda$,
และ $\sigma$ ที่แสดงคือ Basis Function แต่ละอัน โดยทั่วไปแล้ว ในบริบทของการคำนวณ ERI นั้นเราสามารถมองหรือตีความ
$(\mu v \mid$ and $\mid \lambda \sigma)$ ว่าเป็น bra กับ ket ก็ได้

สำหรับ Gaussian Basis Function นั้นเรามีสมการดังต่อไปนี้

\begin{equation}
  \varphi(\boldsymbol{r})
  =
  \sum_{k=1}^K C_k \phi_k\left(\boldsymbol{a}, \boldsymbol{r}, \boldsymbol{A}, \alpha_k\right)
\end{equation}

\noindent และ

\begin{equation}
  \begin{aligned}
    \phi_k\left(\boldsymbol{a}, \boldsymbol{r}, \boldsymbol{A}, \alpha_k\right)
    =
     & N_k\left(x-A_x\right)^{a_x}\left(y-A_y\right)^{a_y}\left(z-A_z\right)^{a_z} \\
     & \times \exp \left(-\alpha_k|\boldsymbol{r}-\boldsymbol{A}|^2\right)
  \end{aligned}
\end{equation}

\noindent ซึ่งสมการด้านบนนั้นจริง ๆ แล้วก็เป็นแค่การเขียนให้ Contracted Gaussian Functions $\varphi(r)$ นั้นอยู่ในรูปของผลรวม%
เชิงเส้นของ Primitive Gaussian Functions $\phi_k\left(\boldsymbol{a}, \boldsymbol{r}, \boldsymbol{A}\right.$, และ
$\left.\alpha_k\right)$ นั่นเอง ซึ่ง Primitive Gaussian Functions ทั้งหมดนั้นจะถูกกำหนดให้มีตำแหน่งจุดศูนย์กลางอยู่บนอะตอม%
ที่ $\boldsymbol{A} = \left(A_x, A_y, A_z\right)$ พร้อมกับมีเลข Orbital Exponent $\alpha_k$ และโมเมนตัมเชิงมุม (Angular 
Momentum) คือ $\boldsymbol{a} = \left(a_x, a_y, a_z\right)$ ส่วน $K$ กับ $C_k$ นั้นจะเป็นตัวที่บ่งบอกถึงอันดับและสัมประสิทธิ์ของการ 
Contraction ส่วนเลข Angular Momentum ของ Basis Function $\varphi(r)$ นั้นมีนิยามคือ $I_a = a_x + a_y + a_z$ 

ในส่วนของ Contracted ERI นั้นสามารถเขียนได้ง่ายกว่านี้โดยการใช้ Summation โดยลูปตามจำนวนของ Primitive Gaussian Functions 
ซึ่งเราสามารถ Represented แทนได้ด้วย Notation ง่าย ๆ คือ $[a b \mid c d]$

\begin{equation}
  (\mu \nu \mid \lambda \sigma) 
  = 
  \sum_{i=1}^{K_a} 
  \sum_{j=1}^{K_b} 
  \sum_{k=1}^{K_c} 
  \sum_{l=1}^{K_d} 
  C_{a i} C_{b j} C_{c k} C_{d l}[a b \mid c d]
\end{equation}

\noindent และ 

\begin{equation}
  [a b \mid c d] 
  = 
  \iint \phi_a\left(\boldsymbol{r}_1\right) \phi_b\left(\boldsymbol{r}_1\right) 
  \frac{1}{r_{12}} 
  \phi_c\left(\boldsymbol{r}_2\right) \phi_d\left(\boldsymbol{r}_2\right) \mathrm{d} \boldsymbol{r}_1 
  \mathrm{~d} \boldsymbol{r}_2
\end{equation}

เนื่องจากว่าการคำนวณ Direct SCF นั้นจะเกี่ยวข้องกับการคำนวณระหว่าง Basis Functions หลายพัน Functions จึงทำให้การคำนวณ ERI 
นั้นเป็นส่วนที่กินเวลานานที่สุดของกระบวนนานทั้งหมด

เนื่องจากว่า ERI นั้นเป็นอินทิกรัล เราจึงสามารถใช้คุณสมบัติความสมมาตรเชิงการสลับที่ของอินทิกรัลเพื่อช่วยลดความสิ้นเปลืองในการคำนวณได้สำหรับ%
การคำนวณแบบขนานบน CPU ดังนี้

\begin{equation}
  \begin{aligned}
    (\mu \nu \mid \lambda \sigma) 
    & = (\mu \nu \mid \sigma \lambda) 
    = (\nu \mu \mid \lambda \sigma) 
    = (\nu \mu \mid \sigma \lambda) \\
    & = (\lambda \sigma \mid \mu \nu) 
    = (\sigma \lambda \mid \mu \nu) 
    = (\lambda \sigma \mid \nu \mu) 
    = (\sigma \lambda \mid \nu \mu)
  \end{aligned}
\end{equation}

\noindent ซึ่งคุณสมบัติดังกล่าวนี้มีชื่อเรียกว่า Eight-Fold Integral Permutational Symmetry ซึ่งจะทำให้เรามี Fock Matrix 
ที่แตกต่างกันทั้งหมด 6 อัน ดังนี้

\begin{equation}
  \begin{aligned}
    F_{\mu \nu} & = F_{\mu \nu}+4 D_{\lambda \sigma}(\mu \nu \mid \lambda \sigma) \\
    F_{\lambda \sigma} & =F_{\lambda \sigma}+4 D_{\mu \nu}(\mu \nu \mid \lambda \sigma) \\
    F_{\mu \lambda} & =F_{\mu \lambda}-D_{\nu \sigma}(\mu \nu \mid \lambda \sigma) \\
    F_{\nu \sigma} & =F_{\nu \sigma}-D_{\mu \lambda}(\mu \nu \mid \lambda \sigma) \\
    F_{\mu \sigma} & =F_{\mu \sigma}-D_{\nu \lambda}(\mu \nu \mid \lambda \sigma) \\
    F_{\nu \lambda} & =F_{\nu \lambda}-D_{\mu \sigma}(\mu \nu \mid \lambda \sigma) 
  \end{aligned}
\end{equation}

ถึงแม้ว่าในปัจจุบันเราจะสามารถใช้เทคนิค Eight-Fold Symmetry มาช่วยในการคำนวณ Fock Matrix บน GPU ได้แล้ว แต่ว่าเรามีปัญหา 2 
อย่างที่ยังต้องแก้ให้ได้คือ

\begin{enumerate}[topsep=0pt,noitemsep]
  \setlength\itemsep{1em}
  \item ยังมี Conflict ที่เกิดขึ้นระหว่างการ Memory Access อยู่ในขั้นตอนที่เราต้องทำการรวม Fock Matrix

  \item เราไม่สามารถเข้าถึง Global Memory เราในขณะที่เรากำลังเชียนหรืออ่านตัว Density Matrix กับ Fock Matrix ที่อยู่ใน Storage 
  เดียวกัน
\end{enumerate}

สำหรับปัญหาอันแรกที่เกี่ยวกับ Memory Access นั้น ได้มีงานวิจัยของ Ufimtsev และ Martínez ที่เสนอให้คำนวณ Coulomb Matrix กับ
Exchange Matrix แบบแยกกันบน GPU โดยการป้องกันปัญหา Memory Access ในการคำนวณ Coulomb Matrix นั้น เราสามารถใช้
Thread Block แต่ละอันเพื่อคำนวณ Single Element ของ Primitive Coulomb Matrix ได้ นอกจากนี้แล้วเราจะไม่ใช้ Integral 
Symmetry ที่เกิดระหว่าง bra กับ ket แต่ว่าเราจะใช้เฉพาะ Symmetry ภายใน bra หรือ ket แยกกันเท่านั้น ซึ่งทำให้เกิดการลดรูปจาก 
Eight-Fold Integral Symmetry เหลือเป็น Four-Fold Integral Symmetry ในการคำนวณ Coulomb Matrix ส่วนการคำนวณ 
Exchange Matrix นั้นก็ทำคล้าย ๆ กัน แต่ว่าเราจะใช้ Two-Fold Symmetry ระหว่าง bra กับ ket แทนเพื่อป้องกันปัญหาการสื่อสารระหว่าง 
Block (Inter-Block Communication)

ที่กล่าวมาด้านบนนั้นเป็น Algorithm ที่เราสามารถ Implement ลงไปบน CPU ได้โดยการแบ่ง Task ออกเป็น Batch ย่อย ๆ คราวนี้เราลองมาดู 
Algorithm สำหรับกรณีของ GPU ซึ่งจะแบ่ง Task ออกเป็นตามจำนวน Kernel แทน ตัวอย่างเช่น การคำนวณ Integrals $(sp \mid sd)$ 
บน CPU นั้นจะสอดคล้องกับการคำนวณ Coulomb Integral $(sp \mid sd)$ และ $(sd \mid sp)$ แล้วก็สอดคล้องกับการคำนวณ 
Exchange Integrals $(sp \mid sd)$, $(sp \mid ds)$, $(ps \mid sd)$ และ $(ps \mid ds)$ บน GPU

สำหรับข้อดีของการใช้ Hybrid Algorithm ก็คือ

\begin{enumerate}[topsep=0pt,noitemsep]
  \setlength\itemsep{1em}
  \item เราสามารถแบ่ง Tasks เพื่อให้ไปคำนวณบน CPU กับ GPU แบบแยกกันได้ซึ่งทำให้การจัดการ Tasks นั้นมีประสิทธิภาพมากขึ้น

  \item การคำนวณ ERI ที่เกิดขึ้นบน CPU กับ GPU นั้นแยกกันอย่างสิ้นเชิง ทำให้ไม่มีปัญหาจากการ Communication ทำให้ Operation 
  นั้นเป็นแบบ Asynchronous แบบสมบูรณ์แบบ
  
  \item วิธีการนี้สามารถนำไปใช้ในการคำนวณ ERI ได้สำหรับ Angular Momentum ทุกอัน
\end{enumerate}

%----------------------------------------
\section{แบบฝึกหัด}
%----------------------------------------
